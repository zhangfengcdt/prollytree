use prollytree::agent::*;
use rig::{completion::Prompt, providers::openai::Client};
use serde::{Deserialize, Serialize};
use serde_json::json;
use std::error::Error;
use tempfile::TempDir;

/// Tools available to the agent, similar to LangGraph example
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AgentTool {
    WriteToScratchpad {
        notes: String,
    },
    ReadFromScratchpad,
    WebSearch {
        query: String,
    },
    StoreFact {
        category: String,
        fact: String,
    },
    StoreRule {
        rule_name: String,
        condition: String,
        action: String,
    },
    RecallFacts {
        category: String,
    },
    RecallRules,
}

/// Tool execution result
#[derive(Debug, Serialize, Deserialize)]
pub struct ToolResult {
    pub tool: AgentTool,
    pub result: String,
}

/// Agent with context offloading capabilities using AgentMemorySystem
pub struct ContextOffloadingAgent {
    memory_system: AgentMemorySystem,
    rig_client: Option<Client>,
    agent_id: String,
    current_thread_id: String,
    namespace: String,
}

impl ContextOffloadingAgent {
    /// Initialize a new agent with persistent memory across threads
    pub async fn new(
        memory_path: &std::path::Path,
        agent_id: String,
        namespace: String,
        openai_api_key: Option<String>,
    ) -> Result<Self, Box<dyn Error>> {
        // Initialize the memory system for cross-thread persistence
        let memory_system = AgentMemorySystem::init(
            memory_path,
            agent_id.clone(),
            Some(Box::new(MockEmbeddingGenerator)),
        )?;

        let rig_client = openai_api_key.map(|key| Client::new(&key));
        let current_thread_id = format!("thread_{}", chrono::Utc::now().timestamp());

        Ok(Self {
            memory_system,
            rig_client,
            agent_id,
            current_thread_id,
            namespace,
        })
    }

    /// Switch to a different conversation thread
    pub fn switch_thread(&mut self, thread_id: String) {
        self.current_thread_id = thread_id;
        println!("üîÑ Switched to thread: {}", self.current_thread_id);
    }

    /// Execute a tool with memory persistence
    pub async fn execute_tool(&mut self, tool: AgentTool) -> Result<ToolResult, Box<dyn Error>> {
        match tool {
            AgentTool::WriteToScratchpad { ref notes } => {
                // Store in semantic memory for cross-thread access
                let memory_id = self
                    .memory_system
                    .semantic
                    .store_fact(
                        "scratchpad",    // entity_type
                        &self.namespace, // entity_id (namespace acts as the specific scratchpad ID)
                        json!({
                            "content": notes,
                            "updated_by": self.current_thread_id,
                            "timestamp": chrono::Utc::now()
                        }),
                        1.0, // max confidence
                        &format!("thread_{}", self.current_thread_id),
                    )
                    .await?;

                println!("üìù Wrote to scratchpad (memory_id: {})", memory_id);

                Ok(ToolResult {
                    tool: tool.clone(),
                    result: format!("Wrote to scratchpad: {}", notes),
                })
            }

            AgentTool::ReadFromScratchpad => {
                // Retrieve from semantic memory across threads
                let facts = self
                    .memory_system
                    .semantic
                    .get_entity_facts("scratchpad", &self.namespace)
                    .await?;

                if !facts.is_empty() {
                    // Get the most recent fact (facts are ordered by timestamp)
                    let latest_fact = facts.last().unwrap();

                    // The content is stored as "fact" field in the semantic memory
                    let content = if let Some(fact_value) = latest_fact.content.get("fact") {
                        // The fact field contains our JSON object with "content"
                        if let Some(fact_obj) = fact_value.as_object() {
                            fact_obj
                                .get("content")
                                .and_then(|c| c.as_str())
                                .unwrap_or("No content found in facts object")
                                .to_string()
                        } else if let Some(fact_str) = fact_value.as_str() {
                            // Try parsing it as JSON string
                            if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(fact_str)
                            {
                                parsed
                                    .get("content")
                                    .and_then(|c| c.as_str())
                                    .unwrap_or("No content found in parsed facts")
                                    .to_string()
                            } else {
                                fact_str.to_string()
                            }
                        } else {
                            "Facts field is not in expected format".to_string()
                        }
                    } else {
                        "No facts field found".to_string()
                    };

                    println!("üìñ Read from scratchpad: {}", content);

                    Ok(ToolResult {
                        tool,
                        result: format!("Notes from scratchpad: {}", content),
                    })
                } else {
                    println!("üìñ No facts found for namespace: {}", self.namespace);
                    Ok(ToolResult {
                        tool,
                        result: "No notes found in scratchpad".to_string(),
                    })
                }
            }

            AgentTool::WebSearch { ref query } => {
                // Simulate web search and store results
                let search_results = format!(
                    "Search results for '{}': Found relevant information about the topic.",
                    query
                );

                // Store search results in episodic memory
                self.memory_system
                    .episodic
                    .store_episode(
                        "search",
                        &format!("Search for: {}", query),
                        json!({
                            "query": query,
                            "results": search_results.clone(),
                            "thread_id": self.current_thread_id
                        }),
                        Some(json!({"success": true})),
                        0.8,
                    )
                    .await?;

                Ok(ToolResult {
                    tool,
                    result: search_results,
                })
            }

            AgentTool::StoreFact {
                ref category,
                ref fact,
            } => {
                // Store as a semantic fact
                let _memory_id = self
                    .memory_system
                    .semantic
                    .store_fact(
                        "research_fact",
                        &format!("{}_{}", self.namespace, category),
                        json!({
                            "category": category,
                            "fact": fact,
                            "stored_by": self.current_thread_id,
                            "timestamp": chrono::Utc::now()
                        }),
                        0.95,
                        &self.current_thread_id,
                    )
                    .await?;

                println!("üìö Stored fact in category '{}': {}", category, fact);

                Ok(ToolResult {
                    tool: tool.clone(),
                    result: format!("Stored fact in {}: {}", category, fact),
                })
            }

            AgentTool::StoreRule {
                ref rule_name,
                ref condition,
                ref action,
            } => {
                // Store as a procedural rule
                self.memory_system
                    .procedural
                    .store_rule(
                        "climate_analysis",
                        rule_name,
                        json!(condition),
                        json!(action),
                        5,
                        true,
                    )
                    .await?;

                println!(
                    "üìè Stored rule '{}': IF {} THEN {}",
                    rule_name, condition, action
                );

                Ok(ToolResult {
                    tool: tool.clone(),
                    result: format!("Stored rule: {}", rule_name),
                })
            }

            AgentTool::RecallFacts { ref category } => {
                // Retrieve facts from semantic memory
                let facts = self
                    .memory_system
                    .semantic
                    .get_entity_facts("research_fact", &format!("{}_{}", self.namespace, category))
                    .await?;

                if !facts.is_empty() {
                    let mut fact_list = Vec::new();
                    for fact in facts.iter() {
                        if let Some(fact_obj) = fact.content.get("fact") {
                            if let Some(fact_data) = fact_obj.as_object() {
                                if let Some(fact_text) =
                                    fact_data.get("fact").and_then(|f| f.as_str())
                                {
                                    fact_list.push(fact_text.to_string());
                                }
                            }
                        }
                    }

                    println!(
                        "üìö Found {} facts in category '{}'",
                        fact_list.len(),
                        category
                    );

                    Ok(ToolResult {
                        tool: tool.clone(),
                        result: if fact_list.is_empty() {
                            format!("No facts found in category: {}", category)
                        } else {
                            format!("Facts in {}: {}", category, fact_list.join("; "))
                        },
                    })
                } else {
                    Ok(ToolResult {
                        tool: tool.clone(),
                        result: format!("No facts found in category: {}", category),
                    })
                }
            }

            AgentTool::RecallRules => {
                // Retrieve rules from procedural memory
                let rules = self
                    .memory_system
                    .procedural
                    .get_active_rules_by_category("climate_analysis")
                    .await?;

                if !rules.is_empty() {
                    let rule_list: Vec<String> = rules
                        .iter()
                        .map(|r| {
                            format!(
                                "{}: {}",
                                r.content
                                    .get("name")
                                    .and_then(|n| n.as_str())
                                    .unwrap_or("Unknown"),
                                r.content
                                    .get("description")
                                    .and_then(|d| d.as_str())
                                    .unwrap_or("")
                            )
                        })
                        .collect();

                    println!("üìè Found {} rules", rule_list.len());

                    Ok(ToolResult {
                        tool,
                        result: format!("Rules: {}", rule_list.join("; ")),
                    })
                } else {
                    Ok(ToolResult {
                        tool,
                        result: "No rules found".to_string(),
                    })
                }
            }
        }
    }

    /// Process a message with tool execution and memory
    pub async fn process_with_tools(&mut self, message: &str) -> Result<String, Box<dyn Error>> {
        // Store the user message in conversation history
        self.memory_system
            .short_term
            .store_conversation_turn(&self.current_thread_id, "user", message, None)
            .await?;

        // Determine which tools to use based on the message
        let tools_to_execute = self.determine_tools(message).await?;

        let mut tool_results = Vec::new();

        // Execute tools
        for tool in tools_to_execute {
            let result = self.execute_tool(tool).await?;
            tool_results.push(result);
        }

        // Generate response based on tool results
        let response = if let Some(ref client) = self.rig_client {
            self.generate_ai_response_with_tools(message, &tool_results, client)
                .await?
        } else {
            self.generate_memory_response_with_tools(message, &tool_results)
                .await?
        };

        // Store assistant response
        self.memory_system
            .short_term
            .store_conversation_turn(&self.current_thread_id, "assistant", &response, None)
            .await?;

        Ok(response)
    }

    /// Use LLM to determine which tools to use based on the message and context
    async fn determine_tools(&self, message: &str) -> Result<Vec<AgentTool>, Box<dyn Error>> {
        // If no LLM client available, fall back to simple parsing
        if self.rig_client.is_none() {
            return self.determine_tools_fallback(message).await;
        }

        let client = self.rig_client.as_ref().unwrap();

        // Get recent conversation context
        let recent_history = self
            .memory_system
            .short_term
            .get_conversation_history(&self.current_thread_id, Some(3))
            .await?;

        let context = recent_history
            .iter()
            .map(|turn| {
                format!(
                    "{}: {}",
                    turn.content
                        .get("role")
                        .and_then(|r| r.as_str())
                        .unwrap_or("unknown"),
                    turn.content
                        .get("content")
                        .and_then(|c| c.as_str())
                        .unwrap_or("")
                )
            })
            .collect::<Vec<_>>()
            .join("\n");

        let tool_selection_prompt = format!(
            r#"You are an AI assistant helping with climate research. Based on the user message and conversation context, select the most appropriate tools from the available options.

AVAILABLE TOOLS:
1. WriteToScratchpad - Store persistent notes accessible across conversation threads
2. ReadFromScratchpad - Retrieve notes from the persistent scratchpad
3. WebSearch - Search for information on the web
4. StoreFact - Store factual information in a specific category (hurricanes, heat_waves, flooding, economic, adaptation)
5. StoreRule - Store conditional rules/policies in IF-THEN format
6. RecallFacts - Retrieve stored facts from a specific category
7. RecallRules - Retrieve all stored rules and policies

RECENT CONVERSATION CONTEXT:
{}

USER MESSAGE: "{}"

INSTRUCTIONS:
- Analyze the user's intent and select 1-3 most appropriate tools
- For StoreFact: extract the fact content and category
- For StoreRule: extract rule name, condition, and action
- For WebSearch: extract the search query
- For RecallFacts: determine the appropriate category
- For WriteToScratchpad: extract the content to remember

Respond ONLY with a JSON object in this exact format:
{{"tools": [{{"name": "ToolName", "params": {{"param1": "value1"}}}}], "reasoning": "brief explanation"}}

Examples:
- "Fact: Miami flood defenses cost $400M category: adaptation" ‚Üí {{"tools": [{{"name": "StoreFact", "params": {{"category": "adaptation", "fact": "Miami flood defenses cost $400M"}}}}], "reasoning": "User wants to store an adaptation fact"}}
- "What hurricanes facts do we have?" ‚Üí {{"tools": [{{"name": "RecallFacts", "params": {{"category": "hurricanes"}}}}], "reasoning": "User wants to recall hurricane facts"}}
- "Search for Atlanta heat data" ‚Üí {{"tools": [{{"name": "WebSearch", "params": {{"query": "Atlanta heat data"}}}}], "reasoning": "User wants to search for information"}}"#,
            context, message
        );

        println!("ü§ñ Using LLM for tool selection...");

        let agent = client
            .agent("gpt-3.5-turbo")
            .preamble(
                "You are a precise tool selection assistant. Always respond with valid JSON only.",
            )
            .max_tokens(300)
            .temperature(0.1)
            .build();

        match agent.prompt(&tool_selection_prompt).await {
            Ok(response) => {
                println!("üìã LLM Response: {}", response.trim());
                self.parse_llm_tool_response(&response).await
            }
            Err(e) => {
                println!("‚ö†Ô∏è LLM tool selection failed: {}, using fallback", e);
                self.determine_tools_fallback(message).await
            }
        }
    }

    /// Parse LLM response and convert to AgentTool instances
    async fn parse_llm_tool_response(
        &self,
        response: &str,
    ) -> Result<Vec<AgentTool>, Box<dyn Error>> {
        let mut tools = Vec::new();

        // Try to parse JSON response
        match serde_json::from_str::<serde_json::Value>(response.trim()) {
            Ok(json) => {
                if let Some(tool_array) = json.get("tools").and_then(|t| t.as_array()) {
                    for tool_obj in tool_array {
                        if let Some(tool_name) = tool_obj.get("name").and_then(|n| n.as_str()) {
                            let default_params = serde_json::json!({});
                            let params = tool_obj.get("params").unwrap_or(&default_params);

                            match tool_name {
                                "WriteToScratchpad" => {
                                    if let Some(notes) =
                                        params.get("notes").and_then(|n| n.as_str())
                                    {
                                        tools.push(AgentTool::WriteToScratchpad {
                                            notes: notes.to_string(),
                                        });
                                    }
                                }
                                "ReadFromScratchpad" => {
                                    tools.push(AgentTool::ReadFromScratchpad);
                                }
                                "WebSearch" => {
                                    if let Some(query) =
                                        params.get("query").and_then(|q| q.as_str())
                                    {
                                        tools.push(AgentTool::WebSearch {
                                            query: query.to_string(),
                                        });
                                    }
                                }
                                "StoreFact" => {
                                    if let (Some(category), Some(fact)) = (
                                        params.get("category").and_then(|c| c.as_str()),
                                        params.get("fact").and_then(|f| f.as_str()),
                                    ) {
                                        tools.push(AgentTool::StoreFact {
                                            category: category.to_string(),
                                            fact: fact.to_string(),
                                        });
                                    }
                                }
                                "StoreRule" => {
                                    if let (Some(rule_name), Some(condition), Some(action)) = (
                                        params.get("rule_name").and_then(|r| r.as_str()),
                                        params.get("condition").and_then(|c| c.as_str()),
                                        params.get("action").and_then(|a| a.as_str()),
                                    ) {
                                        tools.push(AgentTool::StoreRule {
                                            rule_name: rule_name.to_string(),
                                            condition: condition.to_string(),
                                            action: action.to_string(),
                                        });
                                    }
                                }
                                "RecallFacts" => {
                                    if let Some(category) =
                                        params.get("category").and_then(|c| c.as_str())
                                    {
                                        tools.push(AgentTool::RecallFacts {
                                            category: category.to_string(),
                                        });
                                    }
                                }
                                "RecallRules" => {
                                    tools.push(AgentTool::RecallRules);
                                }
                                _ => {
                                    println!("‚ö†Ô∏è Unknown tool name: {}", tool_name);
                                }
                            }
                        }
                    }
                }
            }
            Err(e) => {
                println!("‚ö†Ô∏è Failed to parse LLM JSON response: {}", e);
                // Fall back to simple parsing
                return self.determine_tools_fallback(response).await;
            }
        }

        Ok(tools)
    }

    /// Fallback tool determination when LLM is not available or fails
    async fn determine_tools_fallback(
        &self,
        message: &str,
    ) -> Result<Vec<AgentTool>, Box<dyn Error>> {
        let mut tools = Vec::new();
        let message_lower = message.to_lowercase();

        // Simple fallback logic (original string matching approach)
        if message_lower.contains("recall")
            || message_lower.contains("what did")
            || (message_lower.contains("remember") && message_lower.contains("?"))
        {
            tools.push(AgentTool::ReadFromScratchpad);
            return Ok(tools);
        }

        if message_lower.starts_with("please remember:") || message_lower.starts_with("remember:") {
            let content = if let Some(idx) = message.find(':') {
                message[(idx + 1)..].trim().to_string()
            } else {
                message.to_string()
            };
            tools.push(AgentTool::WriteToScratchpad { notes: content });
        }

        if message_lower.contains("fact:") {
            // Extract fact and category
            let fact_part = message.split("fact:").nth(1).unwrap_or("").trim();
            let (fact, category) = if let Some(cat_idx) = fact_part.to_lowercase().find("category:")
            {
                let fact_text = fact_part[..cat_idx].trim();
                let category_text = fact_part[cat_idx + 9..]
                    .trim()
                    .split_whitespace()
                    .next()
                    .unwrap_or("general");
                (fact_text.to_string(), category_text.to_string())
            } else {
                (fact_part.to_string(), "general".to_string())
            };
            tools.push(AgentTool::StoreFact { category, fact });
        }

        if message_lower.contains("search") {
            let query = message
                .split_whitespace()
                .skip_while(|&w| !w.to_lowercase().contains("search"))
                .skip(1)
                .collect::<Vec<_>>()
                .join(" ");
            if !query.is_empty() {
                tools.push(AgentTool::WebSearch { query });
            }
        }

        Ok(tools)
    }

    /// Generate AI response with tool results
    async fn generate_ai_response_with_tools(
        &self,
        message: &str,
        tool_results: &[ToolResult],
        client: &Client,
    ) -> Result<String, Box<dyn Error>> {
        let tool_context = tool_results
            .iter()
            .map(|tr| format!("Tool: {:?}\nResult: {}", tr.tool, tr.result))
            .collect::<Vec<_>>()
            .join("\n\n");

        let prompt = format!(
            "You are an intelligent assistant with access to various tools and persistent memory.\n\n\
             USER MESSAGE: {}\n\n\
             TOOL EXECUTION RESULTS:\n{}\n\n\
             Based on the tool results, provide a helpful response to the user.",
            message, tool_context
        );

        let agent = client
            .agent("gpt-3.5-turbo")
            .preamble("You are a helpful assistant that uses tools and memory to provide accurate responses.")
            .max_tokens(200)
            .temperature(0.7)
            .build();

        match agent.prompt(&prompt).await {
            Ok(response) => Ok(response.trim().to_string()),
            Err(e) => {
                println!("‚ö†Ô∏è AI generation failed: {}", e);
                self.generate_memory_response_with_tools(message, tool_results)
                    .await
            }
        }
    }

    /// Generate memory-based response with tool results
    async fn generate_memory_response_with_tools(
        &self,
        _message: &str,
        tool_results: &[ToolResult],
    ) -> Result<String, Box<dyn Error>> {
        if tool_results.is_empty() {
            return Ok("I've processed your request. How can I help you further?".to_string());
        }

        let response = tool_results
            .iter()
            .map(|tr| tr.result.clone())
            .collect::<Vec<_>>()
            .join(" ");

        Ok(response)
    }

    /// Get statistics about the agent's memory
    pub async fn get_memory_stats(&self) -> Result<serde_json::Value, Box<dyn Error>> {
        let stats = self.memory_system.get_system_stats().await?;
        Ok(json!({
            "agent_id": self.agent_id,
            "current_thread": self.current_thread_id,
            "namespace": self.namespace,
            "memory_stats": stats
        }))
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    println!("üß† Context Offloading Agent Demo (Rig + ProllyTree)");
    println!("==================================================");
    println!();

    // Create a temporary directory for the demo
    let temp_dir = TempDir::new()?;
    let memory_path = temp_dir.path();

    // Initialize agent
    let mut agent = ContextOffloadingAgent::new(
        memory_path,
        "context_agent_001".to_string(),
        "research_project".to_string(),
        std::env::var("OPENAI_API_KEY").ok(),
    )
    .await?;

    println!("‚úÖ Agent initialized with namespace: research_project");
    println!();

    // Thread 1: Initial research
    println!("üìã Thread 1: Starting research project - Initial Data Collection");
    println!("{}", "‚îÄ".repeat(60));

    let thread1_messages = vec![
        "Please remember: Research project on the impact of extreme weather on southeast US due to climate change. Key areas to track: hurricane intensity trends, flooding patterns, heat wave frequency, economic impacts on agriculture and infrastructure, and adaptation strategies being implemented.",

        "Search for recent data on hurricane damage costs in Florida and Georgia",

        "Fact: Hurricane Ian (2022) caused over $112 billion in damages, making it the costliest natural disaster in Florida's history category: hurricanes",

        "Fact: Category 4 and 5 hurricanes have increased by 25% in the Southeast US since 1980 category: hurricanes",

        "Rule: hurricane_evacuation: IF hurricane category >= 3 AND distance_from_coast < 10_miles THEN mandatory evacuation required",

        "Search for heat wave data in major southeast cities",

        "Fact: Atlanta experienced 35 days above 95¬∞F in 2023, compared to an average of 15 days in the 1990s category: heat_waves",

        "Fact: Heat-related hospitalizations in Southeast US cities have increased by 43% between 2010-2023 category: heat_waves",

        "Rule: heat_advisory: IF temperature > 95F AND heat_index > 105F THEN issue heat advisory and open cooling centers",

        "Search for flooding impact on agriculture in Mississippi Delta",

        "Fact: 2019 Mississippi River flooding caused $6.2 billion in agricultural losses across Arkansas, Mississippi, and Louisiana category: flooding",

        "Rule: flood_insurance: IF property in 100-year floodplain THEN require federal flood insurance for mortgages",
    ];

    for message in thread1_messages {
        println!("üí¨ User: {}", message);
        let response = agent.process_with_tools(message).await?;
        println!("ü§ñ Assistant: {}", response);
        println!();
    }

    // Create checkpoint
    let checkpoint1 = agent
        .memory_system
        .checkpoint("Thread 1 research complete")
        .await?;
    println!("üíæ Created checkpoint: {}", checkpoint1);
    println!();

    // Thread 2: Continue research in new thread
    agent.switch_thread("thread_002".to_string());

    println!("üìã Thread 2: Analysis and Pattern Recognition");
    println!("{}", "‚îÄ".repeat(60));

    let thread2_messages = vec![
        "What did I ask you to remember about my research project?",

        "What facts do we have about hurricanes?",

        "Search for information about heat wave trends in Atlanta and Charlotte over the past decade",

        "Fact: Charlotte's urban heat island effect amplifies temperatures by 5-8¬∞F compared to surrounding areas category: heat_waves",

        "What rules have we established so far?",

        "Rule: agricultural_drought_response: IF rainfall < 50% of normal for 60 days AND crop_stage = critical THEN implement emergency irrigation protocols",

        "Fact: Southeast US coastal property insurance premiums have increased 300% since 2010 due to climate risks category: economic",

        "Search for successful climate adaptation strategies in Miami",

        "Fact: Miami Beach's $400 million stormwater pump system has reduced flooding events by 85% since 2015 category: adaptation",

        "Rule: building_codes: IF new_construction AND flood_zone THEN require elevation minimum 3 feet above base flood elevation",

        "What facts do we have about economic impacts?",
    ];

    for message in thread2_messages {
        println!("üí¨ User: {}", message);
        let response = agent.process_with_tools(message).await?;
        println!("ü§ñ Assistant: {}", response);
        println!();
    }

    // Show memory statistics
    println!("üìä Memory Statistics:");
    println!("{}", "‚ïê".repeat(50));
    let stats = agent.get_memory_stats().await?;
    println!("{}", serde_json::to_string_pretty(&stats)?);
    println!();

    // Thread 3: Demonstrate persistence and synthesis
    agent.switch_thread("thread_003".to_string());

    println!("üìã Thread 3: Synthesis and Policy Recommendations");
    println!("{}", "‚îÄ".repeat(60));

    let thread3_messages = vec![
        "Can you recall what research topics I asked you to track?",

        "What facts do we have about heat waves?",

        "Fact: Federal disaster declarations for heat waves have increased 600% in Southeast US since 2000 category: heat_waves",

        "What are all the rules we've established for climate response?",

        "Fact: Georgia's agricultural sector lost $2.5 billion in 2022 due to extreme weather events category: economic",

        "Rule: infrastructure_resilience: IF critical_infrastructure AND climate_risk_score > 7 THEN require climate resilience assessment and upgrade plan",

        "Search for green infrastructure solutions for urban flooding",

        "Fact: Green infrastructure projects in Atlanta reduced stormwater runoff by 40% and provided $85 million in ecosystem services category: adaptation",

        "What facts have we collected about flooding?",

        "Rule: emergency_response: IF rainfall > 6_inches_24hr OR wind_speed > 75mph THEN activate emergency operations center",

        "Fact: Southeast US has experienced a 40% increase in extreme precipitation events (>3 inches in 24hr) since 1950 category: flooding",

        "What economic impact facts do we have across all categories?",
    ];

    for message in thread3_messages {
        println!("üí¨ User: {}", message);
        let response = agent.process_with_tools(message).await?;
        println!("ü§ñ Assistant: {}", response);
        println!();

        // Small delay for readability
        tokio::time::sleep(tokio::time::Duration::from_millis(300)).await;
    }

    // Final statistics
    println!("üìä Final Memory Statistics:");
    println!("{}", "‚ïê".repeat(50));
    let final_stats = agent.get_memory_stats().await?;
    println!("{}", serde_json::to_string_pretty(&final_stats)?);
    println!();

    println!("üéâ Demo completed successfully!");
    println!();
    println!("Key Features Demonstrated:");
    println!("‚Ä¢ üìù Persistent scratchpad across threads (like LangGraph's memory store)");
    println!(
        "‚Ä¢ üìö Semantic fact storage with categories (hurricanes, heat_waves, flooding, economic)"
    );
    println!("‚Ä¢ üìè Procedural rule storage for climate response policies");
    println!("‚Ä¢ üîÑ Context offloading between conversation threads");
    println!("‚Ä¢ üõ†Ô∏è Seven different tools for comprehensive memory operations");
    println!("‚Ä¢ üß† Cross-thread memory access for facts, rules, and scratchpad");
    println!("‚Ä¢ üìä Memory checkpointing and detailed statistics");
    println!("‚Ä¢ üîç Category-based fact retrieval and rule management");
    println!("‚Ä¢ üíæ Persistent storage of complex research data across sessions");
    println!();

    if std::env::var("OPENAI_API_KEY").is_err() {
        println!("üí° Note: Running in memory-only mode. To enable AI responses:");
        println!("   export OPENAI_API_KEY=your_api_key_here");
        println!("   cargo run --example agent_context_offloading");
    }

    Ok(())
}
