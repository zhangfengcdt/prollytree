# Versioned Memory Architecture for AI Agents with ProllyTree

## Abstract

This document presents an architecture for implementing versioned, auditable memory systems for AI agents using ProllyTree. By treating agent memory as a versioned database, we enable reproducible agent behaviors, memory rollback capabilities, and complete audit trails of agent decision-making processes.

## Table of Contents

1. [Introduction](#introduction)
2. [Memory Architecture Overview](#memory-architecture-overview)
3. [Recommended Technology Stack](#recommended-technology-stack)
4. [Rust Agent Framework Selection](#rust-agent-framework-selection)
5. [Implementation Design](#implementation-design)
6. [Demo Implementation](#demo-implementation)
7. [Use Cases and Examples](#use-cases-and-examples)
8. [Reproducibility Features](#reproducibility-features)
9. [Audit and Debugging Capabilities](#audit-and-debugging-capabilities)
10. [Performance Considerations](#performance-considerations)
11. [Integration Patterns](#integration-patterns)
12. [Future Enhancements](#future-enhancements)

## Introduction

AI agents require sophisticated memory systems to maintain context, learn from interactions, and make informed decisions. Traditional approaches often lack:
- **Reproducibility**: Cannot replay agent behavior from a specific state
- **Auditability**: No clear trail of how memories influenced decisions
- **Rollback**: Cannot undo problematic memory updates
- **Debugging**: Difficult to understand agent behavior evolution

ProllyTree provides a Git-like versioning system for agent memory, enabling time-travel debugging, memory branching, and complete audit trails.

## Memory Architecture Overview

### Hierarchical Memory Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI Agent System                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  Memory Controller                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Short-term  â”‚  â”‚  Long-term  â”‚  â”‚  Episodic   â”‚      â”‚
â”‚  â”‚   Memory    â”‚  â”‚   Memory    â”‚  â”‚   Memory    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                â”‚                â”‚
          â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ProllyTree Versioned Storage               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Working    â”‚  â”‚  Semantic   â”‚  â”‚  Experience â”‚      â”‚
â”‚  â”‚  Context    â”‚  â”‚  Knowledge  â”‚  â”‚   History   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Types

1. **Short-term Memory**: Current conversation context, active goals
2. **Long-term Memory**: Learned facts, user preferences, semantic knowledge
3. **Episodic Memory**: Past interactions, experiences, outcomes

## Recommended Technology Stack

### Core Libraries for Python/General Use

#### 1. **LangChain + ProllyTree**
```python
# Recommended for production AI agents
from langchain.memory import ConversationSummaryBufferMemory
from langchain.schema import BaseMemory
from prollytree import VersionedMemoryStore

class ProllyTreeMemory(BaseMemory):
    """Versioned memory backend for LangChain agents"""
    def __init__(self, store_path: str):
        self.store = VersionedMemoryStore(store_path)
        self.memory_key = "chat_history"
```

#### 2. **LlamaIndex + ProllyTree**
```python
# Best for document-based agents
from llama_index.storage.docstore import BaseDocumentStore
from llama_index.storage.index_store import BaseIndexStore

class ProllyTreeDocStore(BaseDocumentStore):
    """Versioned document storage for RAG applications"""
    def __init__(self, prolly_store):
        self.store = prolly_store
```

#### 3. **MemGPT Architecture**
```python
# For agents requiring sophisticated memory management
from memgpt.memory import CoreMemory, RecallMemory, ArchivalMemory

class VersionedMemGPT:
    def __init__(self, prolly_path: str):
        self.core = ProllyTreeMemory(prolly_path, "core")
        self.recall = ProllyTreeMemory(prolly_path, "recall")
        self.archival = ProllyTreeMemory(prolly_path, "archival")
```

## Rust Agent Framework Selection

### **Recommended: Rig Framework** â­

**Why Rig is the best choice for ProllyTree integration:**

1. **Production Ready**: Most mature and feature-complete Rust AI framework
2. **Type Safety**: Strong type system reduces runtime errors - perfect for reliable demos
3. **Modular Architecture**: Easy to integrate custom memory backends like ProllyTree
4. **Vector Store Integration**: Built-in support for similarity search (needed for memory retrieval)
5. **Async-First**: Designed for high-performance async operations
6. **Active Development**: Well-maintained with good documentation at [rig.rs](https://rig.rs/)

```toml
[dependencies]
rig-core = "0.2.1"
```

### **Alternative Options**

#### **rust-agentai**
- Multiple LLM provider support (OpenAI, Anthropic, Gemini, Ollama)
- Simple API design
- GenAI library integration
- MCP Server support (experimental)

```rust
// Simple usage
let mut agent = Agent::new("You are a useful assistant");
let answer = agent.run("gpt-4o", "Why sky is blue?", None).await?;
```

#### **ai-agents (geminik23)**
- Structured data flow with PipelineNet
- Dynamic flow control for adapting behaviors
- Early stage but flexible architecture

### **Rig Framework Advantages**

- **Type-safe API** reduces runtime errors
- **Unified LLM interface** across providers
- **Advanced AI workflow abstractions**
- **Vector store integration** for semantic search
- **Comprehensive error handling**
- **Modular, scalable architecture**

## Implementation Design

### Storage Schema Design

```sql
-- Short-term memory (conversation context)
CREATE TABLE short_term_memory (
    id INTEGER PRIMARY KEY,
    session_id TEXT,
    timestamp INTEGER,
    role TEXT,  -- 'user', 'assistant', 'system'
    content TEXT,
    embedding BLOB,  -- Vector embedding for similarity search
    metadata JSON
);

-- Long-term semantic memory
CREATE TABLE long_term_memory (
    id INTEGER PRIMARY KEY,
    concept TEXT,
    facts JSON,
    confidence REAL,
    source TEXT,
    created_at INTEGER,
    last_accessed INTEGER,
    access_count INTEGER
);

-- Episodic memory (experiences)
CREATE TABLE episodic_memory (
    id INTEGER PRIMARY KEY,
    episode_id TEXT,
    timestamp INTEGER,
    context JSON,
    action_taken TEXT,
    outcome TEXT,
    reward REAL,
    embedding BLOB
);

-- Memory associations
CREATE TABLE memory_links (
    source_type TEXT,
    source_id INTEGER,
    target_type TEXT,
    target_id INTEGER,
    relation_type TEXT,
    strength REAL
);
```

### Rust Memory Store Interface

```rust
use rig::providers::openai::{Client, GPT_4};
use rig::completion::Prompt;
use prollytree::sql::ProllyStorage;
use serde::{Deserialize, Serialize};
use uuid::Uuid;
use chrono::{DateTime, Utc};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Memory {
    pub id: String,
    pub content: String,
    pub timestamp: DateTime<Utc>,
    pub metadata: serde_json::Value,
    pub embedding: Option<Vec<f32>>,
}

pub struct VersionedMemoryStore {
    storage: ProllyStorage<32>,
    current_session: String,
}

impl VersionedMemoryStore {
    pub async fn new(store_path: &str) -> Result<Self, Box<dyn std::error::Error>> {
        let storage = ProllyStorage::open(std::path::Path::new(store_path))?;
        let mut glue = gluesql_core::prelude::Glue::new(storage.clone());
        
        // Initialize schema
        Self::setup_schema(&mut glue).await?;
        
        Ok(Self {
            storage,
            current_session: Uuid::new_v4().to_string(),
        })
    }
    
    async fn setup_schema(glue: &mut gluesql_core::prelude::Glue<ProllyStorage<32>>) -> Result<(), Box<dyn std::error::Error>> {
        // Create memory tables
        glue.execute(r#"
            CREATE TABLE short_term_memory (
                id TEXT PRIMARY KEY,
                session_id TEXT,
                timestamp INTEGER,
                role TEXT,
                content TEXT,
                metadata TEXT
            )
        "#).await?;
        
        glue.execute(r#"
            CREATE TABLE long_term_memory (
                id TEXT PRIMARY KEY,
                concept TEXT,
                facts TEXT,
                confidence REAL,
                created_at INTEGER,
                access_count INTEGER
            )
        "#).await?;
        
        Ok(())
    }
    
    pub async fn store_memory(&self, memory_type: &str, memory: &Memory) -> Result<String, Box<dyn std::error::Error>> {
        let mut glue = gluesql_core::prelude::Glue::new(self.storage.clone());
        
        match memory_type {
            "short_term" => {
                glue.execute(&format!(r#"
                    INSERT INTO short_term_memory 
                    (id, session_id, timestamp, role, content, metadata)
                    VALUES ('{}', '{}', {}, '{}', '{}', '{}')
                "#, 
                    memory.id,
                    self.current_session,
                    memory.timestamp.timestamp(),
                    memory.metadata.get("role").unwrap_or(&serde_json::Value::String("user".to_string())),
                    memory.content,
                    memory.metadata.to_string()
                )).await?;
            },
            "long_term" => {
                glue.execute(&format!(r#"
                    INSERT INTO long_term_memory 
                    (id, concept, facts, confidence, created_at, access_count)
                    VALUES ('{}', '{}', '{}', 0.8, {}, 1)
                "#,
                    memory.id,
                    memory.metadata.get("concept").unwrap_or(&serde_json::Value::String("general".to_string())),
                    memory.content,
                    memory.timestamp.timestamp()
                )).await?;
            },
            _ => return Err("Unknown memory type".into()),
        }
        
        // Commit changes (this creates a version in ProllyTree)
        // For now, we'll return a placeholder version ID
        Ok(format!("v_{}", chrono::Utc::now().timestamp()))
    }
    
    pub async fn recall_memories(&self, query: &str, memory_type: &str, limit: usize) -> Result<Vec<Memory>, Box<dyn std::error::Error>> {
        let mut glue = gluesql_core::prelude::Glue::new(self.storage.clone());
        
        // Simple text-based recall (in production, would use embeddings)
        let sql = match memory_type {
            "short_term" => format!(r#"
                SELECT id, content, timestamp, metadata 
                FROM short_term_memory 
                WHERE content LIKE '%{}%' 
                ORDER BY timestamp DESC 
                LIMIT {}
            "#, query, limit),
            "long_term" => format!(r#"
                SELECT id, facts as content, created_at as timestamp, concept
                FROM long_term_memory 
                WHERE facts LIKE '%{}%' 
                ORDER BY access_count DESC 
                LIMIT {}
            "#, query, limit),
            _ => return Err("Unknown memory type".into()),
        };
        
        let results = glue.execute(&sql).await?;
        
        // Parse results into Memory structs
        // This is simplified - in practice would properly parse the query results
        Ok(Vec::new()) // Placeholder
    }
}
```

## Demo Implementation

### **Recommended Demo Structure using Rig**

```toml
# Cargo.toml
[package]
name = "prolly-agent-demo"
version = "0.1.0"
edition = "2021"

[dependencies]
rig-core = "0.2.1"
prollytree = { path = "../.." }  # Your local ProllyTree
tokio = { version = "1.0", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
uuid = "1.0"
chrono = { version = "0.4", features = ["serde"] }
anyhow = "1.0"
```

```rust
// src/agent.rs
use rig::providers::openai::{Client, GPT_4};
use rig::completion::Prompt;
use anyhow::Result;

pub struct VersionedAgent {
    llm_client: Client,
    memory_store: VersionedMemoryStore,
    session_id: String,
}

impl VersionedAgent {
    pub async fn new(api_key: String, memory_path: &str) -> Result<Self> {
        let client = Client::new(&api_key);
        let memory_store = VersionedMemoryStore::new(memory_path).await?;
        
        Ok(Self {
            llm_client: client,
            memory_store,
            session_id: uuid::Uuid::new_v4().to_string(),
        })
    }
    
    pub async fn process_message(&mut self, input: &str) -> Result<(String, String)> {
        // 1. Store user input in versioned memory
        let input_memory = Memory {
            id: format!("input_{}", chrono::Utc::now().timestamp()),
            content: input.to_string(),
            timestamp: chrono::Utc::now(),
            metadata: serde_json::json!({"role": "user", "session_id": self.session_id}),
            embedding: None,
        };
        
        self.memory_store.store_memory("short_term", &input_memory).await?;
        
        // 2. Retrieve relevant context from memory
        let context_memories = self.memory_store.recall_memories(input, "long_term", 3).await?;
        let recent_memories = self.memory_store.recall_memories("", "short_term", 5).await?;
        
        // 3. Build prompt with context
        let context_text = self.build_context_text(&context_memories, &recent_memories);
        let prompt = format!(
            "Context from memory:\n{}\n\nUser: {}\nAssistant:",
            context_text, input
        );
        
        // 4. Generate response using Rig
        let response = self.llm_client
            .chat(&prompt, GPT_4)
            .await
            .map_err(|e| anyhow::anyhow!("LLM error: {}", e))?;
        
        // 5. Store response and commit version
        let response_memory = Memory {
            id: format!("response_{}", chrono::Utc::now().timestamp()),
            content: response.clone(),
            timestamp: chrono::Utc::now(),
            metadata: serde_json::json!({
                "role": "assistant", 
                "session_id": self.session_id,
                "context_used": context_memories.len() + recent_memories.len()
            }),
            embedding: None,
        };
        
        let version = self.memory_store.store_memory("short_term", &response_memory).await?;
        
        println!("ğŸ’¾ Memory committed to version: {}", version);
        
        Ok((response, version))
    }
    
    fn build_context_text(&self, long_term: &[Memory], short_term: &[Memory]) -> String {
        let mut context = String::new();
        
        if !long_term.is_empty() {
            context.push_str("Relevant facts:\n");
            for memory in long_term {
                context.push_str(&format!("- {}\n", memory.content));
            }
        }
        
        if !short_term.is_empty() {
            context.push_str("\nRecent conversation:\n");
            for memory in short_term.iter().rev().take(3) {
                let role = memory.metadata.get("role")
                    .and_then(|v| v.as_str())
                    .unwrap_or("unknown");
                context.push_str(&format!("{}: {}\n", role, memory.content));
            }
        }
        
        context
    }
    
    // Memory versioning methods
    pub async fn create_memory_branch(&self, name: &str) -> Result<()> {
        // Create experimental memory branch
        // This would use ProllyTree's branching capabilities
        println!("ğŸŒ¿ Created memory branch: {}", name);
        Ok(())
    }
    
    pub async fn rollback_to_version(&mut self, version: &str) -> Result<()> {
        // Rollback memory to previous state
        println!("âª Rolled back to version: {}", version);
        Ok(())
    }
    
    pub async fn get_current_version(&self) -> String {
        format!("v_{}", chrono::Utc::now().timestamp())
    }
}

// Demo scenarios
impl VersionedAgent {
    pub async fn demo_memory_learning(&mut self) -> Result<()> {
        println!("ğŸ¬ Demo: Memory Learning & Rollback");
        
        // Agent learns user preference
        let (response1, v1) = self.process_message("I prefer technical explanations").await?;
        println!("User: I prefer technical explanations");
        println!("Agent: {}", response1);
        
        // Store this as long-term memory
        let preference_memory = Memory {
            id: format!("pref_{}", chrono::Utc::now().timestamp()),
            content: "User prefers technical explanations".to_string(),
            timestamp: chrono::Utc::now(),
            metadata: serde_json::json!({"concept": "user_preference", "type": "explanation_style"}),
            embedding: None,
        };
        self.memory_store.store_memory("long_term", &preference_memory).await?;
        
        // Agent gives response based on preference
        let (response2, v2) = self.process_message("Explain quantum computing").await?;
        println!("\nUser: Explain quantum computing");
        println!("Agent: {}", response2);
        
        // Now user changes preference (mistake)
        let (response3, v3) = self.process_message("Actually, I prefer simple explanations").await?;
        println!("\nUser: Actually, I prefer simple explanations");
        println!("Agent: {}", response3);
        
        // Rollback to previous preference state
        println!("\nâª Rolling back to version {} (before preference change)", v2);
        self.rollback_to_version(&v2).await?;
        
        // Agent should use original preference again
        let (response4, _) = self.process_message("Explain machine learning").await?;
        println!("\nUser: Explain machine learning");
        println!("Agent: {} (should be technical based on rollback)", response4);
        
        Ok(())
    }
    
    pub async fn demo_branching(&mut self) -> Result<()> {
        println!("\nğŸ¬ Demo: Memory Branching");
        
        // Create experimental personality branch
        self.create_memory_branch("experimental_personality").await?;
        
        // Try different behavior without affecting main memory
        let (response1, _) = self.process_message("You are now very formal and verbose").await?;
        println!("Experimental branch - User: You are now very formal and verbose");
        println!("Agent: {}", response1);
        
        // Switch back to main personality
        println!("ğŸ”„ Switching back to main branch");
        
        // Compare responses from different memory states
        let (response2, _) = self.process_message("Hello, how are you?").await?;
        println!("\nMain branch - User: Hello, how are you?");
        println!("Agent: {} (should be normal personality)", response2);
        
        Ok(())
    }
}
```

```rust
// src/main.rs
mod agent;
mod memory;

use agent::VersionedAgent;
use anyhow::Result;
use std::io::{self, Write};

#[tokio::main]
async fn main() -> Result<()> {
    println!("ğŸ¤– ProllyTree Versioned AI Agent Demo");
    println!("=====================================\n");
    
    // Setup
    let api_key = std::env::var("OPENAI_API_KEY")
        .expect("Please set OPENAI_API_KEY environment variable");
    
    let mut agent = VersionedAgent::new(api_key, "./demo_agent_memory").await?;
    
    // Run demos
    agent.demo_memory_learning().await?;
    agent.demo_branching().await?;
    
    // Interactive mode
    println!("\nğŸ® Interactive Mode (type 'quit' to exit):");
    loop {
        print!("\nYou: ");
        io::stdout().flush()?;
        
        let mut input = String::new();
        io::stdin().read_line(&mut input)?;
        let input = input.trim();
        
        if input.is_empty() || input == "quit" {
            break;
        }
        
        match agent.process_message(input).await {
            Ok((response, version)) => {
                println!("Agent: {}", response);
                println!("ğŸ’¾ Version: {}", version);
            },
            Err(e) => println!("Error: {}", e),
        }
    }
    
    println!("\nğŸ‘‹ Demo completed!");
    Ok(())
}
```

### **Demo Setup Instructions**

1. **Install Rust and dependencies**:
```bash
cargo add rig-core
cargo add prollytree --path ../path/to/prollytree
```

2. **Set up environment**:
```bash
export OPENAI_API_KEY="your-key-here"
```

3. **Run the demo**:
```bash
cargo run
```

### **Demo Scenarios**

#### 1. **Memory Learning & Rollback Demo**
```
ğŸ¬ Demo: Memory Learning & Rollback

User: I prefer technical explanations
Agent: I'll remember your preference for technical explanations...

User: Explain quantum computing
Agent: Quantum computing leverages quantum mechanical phenomena... [technical response]

User: Actually, I prefer simple explanations
Agent: I'll update your preference to simple explanations...

âª Rolling back to version v_1704672400 (before preference change)

User: Explain machine learning  
Agent: Machine learning utilizes statistical algorithms... [technical response based on rollback]
```

#### 2. **Memory Branching Demo**
```
ğŸ¬ Demo: Memory Branching

ğŸŒ¿ Created memory branch: experimental_personality

Experimental branch - User: You are now very formal and verbose
Agent: Indeed, I shall endeavor to communicate with utmost formality...

ğŸ”„ Switching back to main branch

Main branch - User: Hello, how are you?
Agent: Hi! I'm doing well, thanks for asking! [normal personality]
```

#### 3. **Audit Trail Demo**
```
ğŸ” Decision Audit Trail:
- Input: "What's the weather like?"
- Memories accessed: [weather_pref_001, location_data_042]
- Reasoning: Used location preference + weather API
- Confidence: 0.95
- Version: v_1704672500
```

This implementation showcases ProllyTree's versioned memory capabilities with a production-ready Rust agent framework, demonstrating reproducible AI behavior, audit trails, and memory branching for safe experimentation.

## Use Cases and Examples

### 1. Customer Support Agent

```rust
// Initialize versioned memory for support agent
let mut support_agent = VersionedAgent::new(
    api_key,
    "./support_agent_memory"
).await?;

// Customer interaction
let (response, v1) = support_agent.process_message(
    "My order #12345 hasn't arrived yet"
).await?;

// Agent learns about the issue
let issue_memory = Memory {
    id: "customer_issue_12345".to_string(),
    content: serde_json::json!({
        "order_id": "12345", 
        "issue": "not_delivered", 
        "reported_date": "2024-01-15"
    }).to_string(),
    timestamp: chrono::Utc::now(),
    metadata: serde_json::json!({"customer_id": "cust_789"}),
    embedding: None,
};

support_agent.memory_store.store_memory("long_term", &issue_memory).await?;

// Later interaction - agent remembers
let (response, v2) = support_agent.process_message(
    "Any update on my order?"
).await?;
// Agent recalls previous context about order #12345
```

### 2. Personal Assistant with Rollback

```rust
// Personal assistant learns user preferences
let mut assistant = VersionedAgent::new(
    api_key,
    "./personal_assistant"
).await?;

// User teaches preference
let (_, v1) = assistant.process_message(
    "I prefer my meetings scheduled in the afternoon"
).await?;

// Assistant makes a mistake
let (_, v2) = assistant.process_message(
    "Schedule all my meetings at 9 AM from now on"
).await?;

// User wants to rollback the incorrect preference
assistant.rollback_to_version(&v1).await?;
assistant.create_memory_branch("corrected_preferences").await?;

// Continue with original preferences
let (response, v3) = assistant.process_message(
    "When should I schedule your meeting with Bob?"
).await?;
// Response: "I'll schedule it for the afternoon, as you prefer"
```

### 3. Research Assistant with Branching

```rust
// Research assistant exploring different hypotheses
let mut research_agent = VersionedAgent::new(
    api_key,
    "./research_memory"
).await?;

// Main research thread
research_agent.process_message(
    "Analyze the relationship between A and B"
).await?;

// Create branch for alternative hypothesis
research_agent.create_memory_branch("hypothesis_2").await?;

// Explore alternative without affecting main research
research_agent.process_message(
    "What if A and B are inversely related?"
).await?;

// Compare conclusions from different branches
let main_conclusion = research_agent.get_response_from_branch("main", "Summarize findings").await?;
let alt_conclusion = research_agent.get_response_from_branch("hypothesis_2", "Summarize findings").await?;
```

## Reproducibility Features

### Deterministic Replay

```rust
impl VersionedAgent {
    pub async fn replay_from_version(&mut self, version: &str, inputs: Vec<&str>) -> Result<Vec<String>> {
        // Checkout historical state
        self.memory_store.checkout_version(version).await?;
        
        // Create isolated branch for replay
        let replay_branch = format!("replay_{}", chrono::Utc::now().timestamp());
        self.create_memory_branch(&replay_branch).await?;
        
        let mut responses = Vec::new();
        for input_text in inputs {
            let (response, _) = self.process_message(input_text).await?;
            responses.push(response);
        }
        
        // Return to main branch
        self.memory_store.checkout_version("main").await?;
        Ok(responses)
    }
    
    pub async fn compare_behaviors(&mut self, input_text: &str, versions: Vec<&str>) -> Result<std::collections::HashMap<String, String>> {
        let mut results = std::collections::HashMap::new();
        
        for version in versions {
            self.memory_store.checkout_version(version).await?;
            let (response, _) = self.process_message(input_text).await?;
            results.insert(version.to_string(), response);
        }
        
        self.memory_store.checkout_version("main").await?;
        Ok(results)
    }
}
```

### Memory Snapshots

```rust
// Create named snapshots for important states
agent.memory_store.tag_version("v1.0", "Initial personality configuration").await?;
agent.memory_store.tag_version("v1.1", "After customer service training").await?;
agent.memory_store.tag_version("v2.0", "Major behavior update").await?;

// Restore to specific snapshot
agent.memory_store.checkout_version("v1.1").await?;
```

## Audit and Debugging Capabilities

### Decision Audit Trail

```rust
#[derive(Debug, Serialize, Deserialize)]
pub struct DecisionAudit {
    pub timestamp: DateTime<Utc>,
    pub input: String,
    pub memories_accessed: Vec<String>,
    pub reasoning_chain: Vec<String>,
    pub decision: String,
    pub confidence: f64,
    pub version: String,
}

impl VersionedAgent {
    pub async fn make_auditable_decision(&mut self, input_text: &str) -> Result<DecisionAudit> {
        let mut memories_accessed = Vec::new();
        
        // Track memory access during processing
        let context_memories = self.memory_store.recall_memories(input_text, "long_term", 3).await?;
        memories_accessed.extend(context_memories.iter().map(|m| m.id.clone()));
        
        let recent_memories = self.memory_store.recall_memories("", "short_term", 5).await?;
        memories_accessed.extend(recent_memories.iter().map(|m| m.id.clone()));
        
        // Process with tracking
        let (response, version) = self.process_message(input_text).await?;
        
        // Extract reasoning (simplified)
        let reasoning_chain = vec![
            format!("Accessed {} long-term memories", context_memories.len()),
            format!("Accessed {} recent memories", recent_memories.len()),
            "Generated response using GPT-4".to_string(),
        ];
        
        let audit = DecisionAudit {
            timestamp: chrono::Utc::now(),
            input: input_text.to_string(),
            memories_accessed,
            reasoning_chain,
            decision: response,
            confidence: 0.85, // Would calculate based on model confidence
            version,
        };
        
        // Store audit trail
        let audit_memory = Memory {
            id: format!("audit_{}", chrono::Utc::now().timestamp()),
            content: serde_json::to_string(&audit)?,
            timestamp: chrono::Utc::now(),
            metadata: serde_json::json!({"type": "audit_log"}),
            embedding: None,
        };
        
        self.memory_store.store_memory("episodic", &audit_memory).await?;
        
        Ok(audit)
    }
}
```

### Memory Diff Analysis

```rust
pub async fn analyze_memory_changes(
    memory_store: &VersionedMemoryStore,
    from_version: &str,
    to_version: &str
) -> Result<serde_json::Value> {
    // Get memory diff using ProllyTree's diff capabilities
    let diff = memory_store.diff_versions(from_version, to_version).await?;
    
    let mut analysis = serde_json::json!({
        "total_changes": 0,
        "memories_added": [],
        "memories_modified": [],
        "memories_removed": [],
        "concepts_learned": [],
        "preferences_changed": []
    });
    
    // Analyze each change
    for change in diff {
        match change.operation.as_str() {
            "added" => {
                analysis["memories_added"].as_array_mut().unwrap().push(serde_json::json!(change));
                if change.data.get("concept").is_some() {
                    analysis["concepts_learned"].as_array_mut().unwrap().push(change.data["concept"].clone());
                }
            },
            "modified" => {
                analysis["memories_modified"].as_array_mut().unwrap().push(serde_json::json!(change));
            },
            "removed" => {
                analysis["memories_removed"].as_array_mut().unwrap().push(serde_json::json!(change));
            },
            _ => {}
        }
    }
    
    analysis["total_changes"] = serde_json::json!(
        analysis["memories_added"].as_array().unwrap().len() +
        analysis["memories_modified"].as_array().unwrap().len() +
        analysis["memories_removed"].as_array().unwrap().len()
    );
    
    Ok(analysis)
}
```

## Performance Considerations

### Memory Indexing

```sql
-- Optimize for semantic search
CREATE INDEX idx_embedding_short ON short_term_memory(embedding);
CREATE INDEX idx_embedding_long ON long_term_memory(embedding);
CREATE INDEX idx_embedding_episodic ON episodic_memory(embedding);

-- Optimize for temporal queries
CREATE INDEX idx_timestamp ON short_term_memory(timestamp);
CREATE INDEX idx_episode_time ON episodic_memory(timestamp);

-- Optimize for memory links
CREATE INDEX idx_links_source ON memory_links(source_type, source_id);
CREATE INDEX idx_links_target ON memory_links(target_type, target_id);
```

### Chunking Strategy

```rust
impl VersionedMemoryStore {
    pub async fn store_memory_batch(&self, memory_type: &str, memories: Vec<Memory>) -> Result<String> {
        let chunk_size = 1000;
        
        // Batch insert for performance
        for chunk in memories.chunks(chunk_size) {
            for memory in chunk {
                self.store_memory(memory_type, memory).await?;
            }
        }
        
        // Single commit for entire batch
        Ok(format!("batch_{}", chrono::Utc::now().timestamp()))
    }
}
```

### Memory Pruning

```rust
pub async fn prune_old_memories(
    memory_store: &VersionedMemoryStore,
    retention_days: i64
) -> Result<()> {
    let cutoff_timestamp = chrono::Utc::now().timestamp() - (retention_days * 24 * 60 * 60);
    
    let mut glue = gluesql_core::prelude::Glue::new(memory_store.storage.clone());
    
    // Archive important memories to episodic before deletion
    glue.execute(&format!(r#"
        INSERT INTO episodic_memory 
        SELECT id, session_id as episode_id, timestamp, 'archived' as context, 
               content as action_taken, 'pruned' as outcome, 0.5 as reward, null as embedding
        FROM short_term_memory 
        WHERE timestamp < {} AND metadata LIKE '%important%'
    "#, cutoff_timestamp)).await?;
    
    // Delete old short-term memories
    glue.execute(&format!(r#"
        DELETE FROM short_term_memory WHERE timestamp < {}
    "#, cutoff_timestamp)).await?;
    
    println!("ğŸ§¹ Pruned memories older than {} days", retention_days);
    Ok(())
}
```

## Integration Patterns

### Rig Framework Integration

```rust
use rig::{
    completion::Prompt,
    providers::openai::{Client, GPT_4},
    vector_store::{VectorStore, VectorStoreIndex},
};

pub struct RigProllyAgent {
    client: Client,
    memory_store: VersionedMemoryStore,
    vector_index: Option<Box<dyn VectorStoreIndex>>,
}

impl RigProllyAgent {
    pub async fn new(api_key: String, memory_path: &str) -> Result<Self> {
        let client = Client::new(&api_key);
        let memory_store = VersionedMemoryStore::new(memory_path).await?;
        
        Ok(Self {
            client,
            memory_store,
            vector_index: None, // Could integrate with Rig's vector stores
        })
    }
    
    pub async fn chat_with_memory(&mut self, message: &str) -> Result<String> {
        // Retrieve context using Rig's vector capabilities if available
        let context = if let Some(index) = &self.vector_index {
            index.top_n::<String>(message, 5).await?
        } else {
            // Fallback to simple text search
            self.memory_store.recall_memories(message, "short_term", 5).await?
                .into_iter()
                .map(|m| m.content)
                .collect()
        };
        
        // Build prompt with context
        let prompt = Prompt::new(&format!(
            "Context: {}\n\nUser: {}\nAssistant:", 
            context.join("\n"), 
            message
        ));
        
        // Use Rig's completion API
        let response = self.client
            .completion_request(GPT_4, prompt)
            .await?;
        
        // Store in versioned memory
        let memory = Memory {
            id: format!("resp_{}", chrono::Utc::now().timestamp()),
            content: response.clone(),
            timestamp: chrono::Utc::now(),
            metadata: serde_json::json!({"role": "assistant"}),
            embedding: None,
        };
        
        self.memory_store.store_memory("short_term", &memory).await?;
        
        Ok(response)
    }
}
```

## Future Enhancements

### 1. **Distributed Agent Memory**
```rust
// Multiple agents sharing versioned memory
pub struct DistributedAgentMemory {
    local_store: VersionedMemoryStore,
    remote_backend: String, // s3://agent-memories/
}

impl DistributedAgentMemory {
    pub async fn sync_memories(&self, agent_id: &str) -> Result<()> {
        // Pull latest changes from shared memory
        self.pull_changes(agent_id).await?;
        
        // Push local changes
        self.push_changes(agent_id).await?;
        
        Ok(())
    }
}
```

### 2. **Memory Compression**
```rust
// Automatically compress and summarize old memories
impl VersionedMemoryStore {
    pub async fn compress_memories(&self, older_than_days: i64) -> Result<()> {
        let old_conversations = self.get_old_conversations(older_than_days).await?;
        
        for conv in old_conversations {
            let summary = self.summarize_conversation(&conv).await?;
            
            let compressed_memory = Memory {
                id: format!("summary_{}", conv.id),
                content: summary,
                timestamp: chrono::Utc::now(),
                metadata: serde_json::json!({
                    "type": "compressed",
                    "original_ids": conv.message_ids,
                    "compression_ratio": summary.len() as f64 / conv.total_length as f64
                }),
                embedding: None,
            };
            
            self.store_memory("compressed", &compressed_memory).await?;
        }
        
        Ok(())
    }
}
```

### 3. **Embedding-Based Memory Retrieval**
```rust
// Use actual embeddings for semantic search
impl VersionedMemoryStore {
    pub async fn recall_by_embedding(&self, query_embedding: Vec<f32>, top_k: usize) -> Result<Vec<Memory>> {
        // This would integrate with a vector database or embedding service
        // For now, placeholder implementation
        
        let mut glue = gluesql_core::prelude::Glue::new(self.storage.clone());
        
        // In practice, would use cosine similarity on embeddings
        let results = glue.execute(&format!(r#"
            SELECT * FROM short_term_memory 
            ORDER BY id DESC 
            LIMIT {}
        "#, top_k)).await?;
        
        // Parse and return memories
        Ok(Vec::new()) // Placeholder
    }
}
```

### 4. **Memory Attention Mechanisms**
```rust
// Implement attention-based memory retrieval
pub struct AttentionMemoryRetriever {
    memory_store: VersionedMemoryStore,
    attention_model: Box<dyn AttentionModel>,
}

impl AttentionMemoryRetriever {
    pub async fn attend_to_memories(&self, query: &str, context: &[Memory]) -> Result<Vec<Memory>> {
        // Compute attention scores between query and all memories
        let scores = self.attention_model.compute_attention(query, context).await?;
        
        // Sort by attention weight and return top memories
        let mut scored_memories: Vec<(f64, &Memory)> = scores.iter()
            .zip(context.iter())
            .collect();
        
        scored_memories.sort_by(|a, b| b.0.partial_cmp(&a.0).unwrap());
        
        Ok(scored_memories.into_iter()
            .take(5)
            .map(|(_, memory)| memory.clone())
            .collect())
    }
}
```

## Conclusion

Versioned memory using ProllyTree transforms AI agents from stateless response generators into intelligent systems with auditable, reproducible behavior. Key benefits include:

1. **Complete Audit Trails**: Every decision can be traced to specific memories
2. **Reproducible Behavior**: Agents can be reset to any historical state
3. **Safe Experimentation**: Branch memories for testing without affecting production
4. **Collaborative Learning**: Multiple agents can share and merge memories
5. **Regulatory Compliance**: Maintain required audit logs and data lineage

This architecture is particularly valuable for:
- Customer service agents requiring consistency
- Medical diagnosis assistants needing audit trails
- Financial advisors with compliance requirements
- Research assistants exploring multiple hypotheses
- Personal assistants learning user preferences

By treating agent memory as a versioned database, we enable a new generation of AI systems that are not only intelligent but also accountable and debuggable. The **Rig framework** provides the ideal foundation for building production-ready demos that showcase these capabilities with type safety and performance.